# ÔªøCMD LOCAL AI


## üìù Short Description

### English:
Local AI app for running GGUF models with `llama-cpp-python`.


### Polish:
Lokalna aplikacja AI do uruchamiania modeli GGUF przez `llama-cpp-python`.

## üîß Instructions

## English

### Quick Start

1. Python 3.11+ required.
2. Install a GGUF model and copy it to models/.
3. Run the application:
`py -3 run.py`
4. Load the model:
`load <filename.gguf>`
5. Talk to the model:
`ai`
or:
`chat <text>`

### CLI commands

- `load` - choose model from list
- `load <name>` - load model by filename
- `models` - list local GGUF models
- `catalog` - list downloadable aliases
- `download <alias|url> [file.gguf]` - download model
- `pull <alias|url> [file.gguf]` - alias for `download`
- `ai` - chat mode (`AI>`)
- `chat <text>` - single message
- `unload` - unload model from RAM
- `status` - app status
- `version` - `llama-cpp-python` version
- `update` - runtime update

### SERVER (localhost:8080)

- `GET /tags`
- `POST /generate`
- `POST /chat`
- `POST /pull`
- `GET /version`

## Polish

### Szybki start

1. Wymagany Python 3.11+.
2.zainstaluj model gguf i skopiuj do `models/`
3. Uruchom aplikacje:
   `py -3 run.py`
4. Zaladuj model:
   `load <nazwa_pliku.gguf>`
5. Rozmawiaj z modelem:
   `ai`
   lub:
   `chat <tekst>`

### Komendy CLI

- `load` - wybor modelu z listy
- `load <name>` - ladowanie modelu po nazwie pliku
- `models` - lista modeli GGUF
- `catalog` - lista aliasow do pobierania
- `download <alias|url> [file.gguf]` - pobieranie modelu
- `pull <alias|url> [file.gguf]` - alias dla `download`
- `ai` - tryb rozmowy (`AI>`)
- `chat <tekst>` - pojedyncza wiadomosc
- `unload` - zwolnienie modelu z RAM
- `status` - status aplikacji
- `version` - wersja `llama-cpp-python`
- `update` - aktualizacja runtime

### SERWER (localhost:8080)

- `GET /tags`
- `POST /generate`
- `POST /chat`
- `POST /pull`
- `GET /version`

---

## Notes:

### English

- `models/` and large local binaries are excluded from the repository by `.gitignore`.
- This project is designed to run fully local models.
- The application can only work with the `models/` file and the `run.py` file

### Polish

- `models/` i du≈ºe lokalne pliki binarne sƒÖ wykluczane z repozytorium przez `.gitignore`.
- Ten projekt zosta≈Ç zaprojektowany do uruchamiania w pe≈Çni lokalnych modeli.
- Aplikacja dzia≈Ça z katalogiem models/ i plikiem run.py.

---

## ‚≠ê Inspirations

### English

This project was inspired by tools like **Ollama** and **LM Studio**, which showed how powerful and accessible local AI models can be.  
I wanted to bring a similar experience directly into the command line ‚Äî simple, fast, and fully local ‚Äî without the need for heavy interfaces or external services.  
CMD LOCAL AI is my attempt to create a clean, terminal‚Äënative way to interact with AI models.

### Polish

InspiracjƒÖ dla tego projektu by≈Çy narzƒôdzia takie jak **Ollama** i **LM Studio**, kt√≥re pokaza≈Çy, jak potƒô≈ºne i dostƒôpne mogƒÖ byƒá lokalne modele AI.
Chcia≈Çem przenie≈õƒá podobne ≈õrodowisko bezpo≈õrednio do wiersza polece≈Ñ ‚Äî proste, szybkie i w pe≈Çni lokalne ‚Äî bez potrzeby stosowania rozbudowanych interfejs√≥w ani us≈Çug zewnƒôtrznych.
CMD LOCAL AI to moja pr√≥ba stworzenia przejrzystego, natywnego dla terminala sposobu interakcji z modelami AI.

---

## üöÄ Future Ideas

### English
CMD LOCAL AI is a project being developed step by step. Below are the directions I plan to develop in future versions:

- **Support for multiple models simultaneously** - switching between local models without restarting the application.
- **Performance profiling** - real-time viewing of generation time and RAM and CPU usage.
- **Developer Tools mode** - viewing system prompts, tokens, and raw model responses.
- **Integration with plugins/extensions** - the ability to add custom commands and actions performed by the AI.
- **Support for audio and vision models** - speech and image support for compatible local models.
  
### Polish

CMD LOCAL AI to projekt rozwijany krok po kroku. Poni≈ºej kierunki, kt√≥re planujƒô rozwijaƒá w kolejnych wersjach:

- **Wsparcie dla wielu modeli jednocze≈õnie** - prze≈ÇƒÖczanie miƒôdzy lokalnymi modelami bez restartu aplikacji.
- **Profilowanie wydajno≈õci** - podglƒÖd czasu generowania oraz zu≈ºycia RAM i CPU w czasie rzeczywistym.
- **Tryb Developer Tools** - podglƒÖd prompt√≥w systemowych, token√≥w i surowych odpowiedzi modelu.
- **Integracja z pluginami / rozszerzeniami** - mo≈ºliwo≈õƒá dodawania w≈Çasnych komend i akcji wykonywanych przez AI.
- **Wsparcie dla modeli audio i vision** - obs≈Çuga mowy i obraz√≥w dla kompatybilnych modeli lokalnych.

---

### THANKS FOR READING :)












